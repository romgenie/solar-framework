"""
This module implements the Hybrid Scaling component of the SOLAR framework.
Hybrid Scaling combines training scaling (via Topological Tuning) with inference scaling 
(Topological Rewarding) to achieve higher reasoning accuracy.

Components included:
- FineTunedModel: A simulated fine-tuned language model optimized through topological tuning.
- MultiTaskTopologicalRewardModel: Evaluates and assigns scores to responses generated by the fine-tuned model.
- HybridScalingInference: Integrates the fine-tuned model and reward model to select the optimal response.
"""

import random

class FineTunedModel:
    """
    Simulated Fine-Tuned Model that has been optimized using Topological Tuning.
    
    This model is assumed to generate more accurate and concise responses for each reasoning topology.
    """
    def generate_response(self, problem_statement, topology):
        """
        Generate a fine-tuned response for the given problem statement and reasoning topology.
        
        Parameters:
            problem_statement (str): The input problem statement.
            topology (str): The reasoning topology to be used, e.g., "Chain-of-Thought",
                            "Tree-of-Thought", or "Graph-of-Thought".
                            
        Returns:
            str: The fine-tuned response simulating improved reasoning.
        """
        if topology == "Chain-of-Thought":
            return f"Fine-tuned CoT response for '{problem_statement}': Step 1 -> Step 2 -> Final Answer."
        elif topology == "Tree-of-Thought":
            return f"Fine-tuned ToT response for '{problem_statement}': Branch A -> Branch B -> Merged Conclusion."
        elif topology == "Graph-of-Thought":
            return f"Fine-tuned GoT response for '{problem_statement}': Node 1 <-> Node 2 -> Final Answer."
        else:
            return f"Unknown topology for '{problem_statement}'."

class MultiTaskTopologicalRewardModel:
    """
    Multi-task Topological Reward Model (M-TRM) that scores responses based on their quality.
    
    In the context of Hybrid Scaling, this model evaluates the responses generated by the fine-tuned model.
    """
    def __init__(self):
        """Initialize the reward model with default parameters."""
        # Import required similarity modules on-demand
        try:
            from difflib import SequenceMatcher
            self.sequence_matcher = SequenceMatcher
        except ImportError:
            self.sequence_matcher = None
            
        self.ground_truth = None
    
    def set_ground_truth(self, ground_truth):
        """
        Set the ground truth answer for evaluating responses.
        
        Parameters:
            ground_truth (str): The reference answer to compare against.
        """
        self.ground_truth = ground_truth
    
    def extract_answer(self, response):
        """
        Extract the final answer from a response using simple heuristics.
        
        Parameters:
            response (str): The full response text.
            
        Returns:
            str: The extracted answer.
        """
        # Look for common answer indicators
        answer_indicators = [
            "The answer is", "Therefore,", "Thus,", "So,", "Final answer:",
            "Therefore the answer is", "In conclusion,", "The result is"
        ]
        
        # Try to find the answer after these indicators
        for indicator in answer_indicators:
            if indicator in response:
                answer_part = response.split(indicator, 1)[1].strip()
                # Take just the first sentence of the answer part
                if '.' in answer_part:
                    return answer_part.split('.', 1)[0].strip()
                else:
                    return answer_part.strip()[:100]  # Limit length
        
        # If no indicators found, take the last sentence
        sentences = response.split('.')
        if len(sentences) > 1:
            return sentences[-2].strip()  # Often the last sentence is just empty or a signature
        
        # Fallback: just return a truncated version of the full text
        return response[:100] + "..."
    
    def compute_string_similarity(self, text1, text2):
        """
        Compute string similarity between two texts using SequenceMatcher.
        
        Parameters:
            text1 (str): First text.
            text2 (str): Second text.
            
        Returns:
            float: Similarity score between 0 and 1.
        """
        if not text1 or not text2:
            return 0.0
            
        if self.sequence_matcher:
            # Use SequenceMatcher for string similarity
            matcher = self.sequence_matcher(None, text1.lower(), text2.lower())
            return matcher.ratio()
        else:
            # Fallback to a simpler similarity measure
            text1_words = set(text1.lower().split())
            text2_words = set(text2.lower().split())
            if not text1_words or not text2_words:
                return 0.0
                
            intersection = text1_words.intersection(text2_words)
            union = text1_words.union(text2_words)
            return len(intersection) / len(union)
    
    def score_response(self, response, ground_truth=None):
        """
        Compute a reward score for a given fine-tuned reasoning response.
        
        Parameters:
            response (str): The fine-tuned reasoning response.
            ground_truth (str, optional): The ground truth answer to compare against.
            
        Returns:
            float: The computed reward score.
        """
        # Use provided ground truth or the one set for the model
        truth = ground_truth if ground_truth is not None else self.ground_truth
        
        # If no ground truth is available, fall back to random scores
        if not truth:
            return random.uniform(0, 1)
            
        # Extract the answer from the response
        extracted_answer = self.extract_answer(response)
        
        # Compute similarity between extracted answer and ground truth
        similarity = self.compute_string_similarity(extracted_answer, truth)
        
        # Return a score that combines similarity with a small random component
        # This helps break ties and adds some exploration to topology selection
        return similarity * 0.9 + random.uniform(0, 0.1)
    
    def select_optimal_response(self, responses, ground_truth=None):
        """
        Select the optimal response among responses from various topologies.
        
        Parameters:
            responses (dict): A dictionary with topology names as keys and responses as values.
            ground_truth (str, optional): The ground truth answer to compare against.
            
        Returns:
            tuple: Contains:
                - selected_topology (str): The topology with the highest reward score.
                - selected_response (str): The corresponding fine-tuned response.
                - score (float): The reward score for the selected response.
                - scores (dict): The scores for all topologies.
        """
        scores = {}
        for topology, response in responses.items():
            scores[topology] = self.score_response(response, ground_truth)
        
        selected_topology = max(scores, key=scores.get)
        selected_score = scores[selected_topology]
        return selected_topology, responses[selected_topology], selected_score, scores

class HybridScalingInference:
    """
    Hybrid Scaling Inference module that integrates the fine-tuned model with the inference reward model.
    
    This component generates responses using different reasoning topologies from the fine-tuned model
    and then applies the reward model to select the best response.
    """
    def __init__(self):
        """
        Initialize the HybridScalingInference module with an instance of the fine-tuned model 
        and the multi-task topological reward model.
        """
        self.fine_tuned_model = FineTunedModel()
        self.reward_model = MultiTaskTopologicalRewardModel()
        self.topologies = ["Chain-of-Thought", "Tree-of-Thought", "Graph-of-Thought"]
    
    def process_request(self, problem_statement, ground_truth=None):
        """
        Process a problem statement by generating fine-tuned responses across all reasoning topologies,
        and selecting the optimal response based on reward scores.
        
        Parameters:
            problem_statement (str): The problem statement to solve.
            ground_truth (str, optional): The ground truth answer for verification.
            
        Returns:
            dict: Contains:
                - selected_topology (str): The topology with the optimal fine-tuned response.
                - response (str): The selected fine-tuned reasoning response.
                - score (float): The reward score for the selected response.
                - all_responses (dict): All generated responses by topology.
                - extracted_answer (str): The answer extracted from the selected response.
                - accuracy (float): Similarity score to ground truth (if provided).
                - topology_scores (dict): Scores for each topology.
        """
        responses = {}
        for topology in self.topologies:
            responses[topology] = self.fine_tuned_model.generate_response(problem_statement, topology)
        
        # Set ground truth for reward model if provided
        if ground_truth:
            self.reward_model.set_ground_truth(ground_truth)
        
        # Select the best response based on reward model
        selected_topology, selected_response, score, topology_scores = self.reward_model.select_optimal_response(responses, ground_truth)
        
        # Extract the answer from the selected response
        extracted_answer = None
        accuracy = 0.0
        if ground_truth:
            extracted_answer = self.reward_model.extract_answer(selected_response)
            accuracy = self.reward_model.compute_string_similarity(extracted_answer, ground_truth)
        
        return {
            'selected_topology': selected_topology,
            'response': selected_response,
            'score': score,
            'all_responses': responses,
            'extracted_answer': extracted_answer,
            'accuracy': accuracy,
            'topology_scores': topology_scores
        }

if __name__ == "__main__":
    # Example usage of the Hybrid Scaling Inference module
    problem = "Solve the math problem: What is 3 * 3?"
    hybrid_inference = HybridScalingInference()
    result = hybrid_inference.process_request(problem)
    
    print("Selected Topology:", result['selected_topology'])
    print("Response:", result['response'])
    print("Score:", result['score'])
    print("All Responses:", result['all_responses'])